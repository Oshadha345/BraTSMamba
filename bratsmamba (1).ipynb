{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":289345111,"sourceType":"kernelVersion"}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# BratsMamba: 3D Brain Tumor Segmentation with State Space Model\n\n","metadata":{}},{"cell_type":"markdown","source":"## Introduction\n\nOur main goal is to capture both fine-grained tumor boundaries (Necrosis) and global contextual features (Edema) efficiently in MRI tumor images. This is a heavily researched and critical area vital for bio-medical engineering field. This problem is in the domain of computer vision, specifically semantic segmentation tasks. \n\nSemantic segmentation is a critical task in computer vision, requiring the precise classification of every pixel in an image. Traditional Convolutional Neural Networks (CNNs) like U-Net excel at capturing local features but often struggle with long-range dependencies due to their limited receptive fields. Transformers addressed this with self-attention, but at the cost of quadratic computational complexity($O(N^2)$), making them heavy for high-resolution tasks.\n\n**Why Mamba?** We address this trade-off using **Mamba-SSM (State Space Models)**. Mamba offers linear complexity ($O(N)$) with respect to sequence length, allowing us to model global context (long-range dependencies) without the massive memory overhead of Transformers.\n\n**The Solution: BratsMamba** \nThis notebook implements BratsMamba, a hybrid architecture that combines the hierarchical structure of a U-Net with Mamba blocks. This allows us to capture:\n1. **Local Texture Details**: Via convolutional stems and decoder blocks.\n3. **Global Semantic Context**: Via Mamba encoders that scan the image as a sequence, understanding the \"whole picture\" efficiently.\n\n### Key Technical Features:\n* **Architecture:** Dual-Path Conv Stem + Mamba Encoder/Decoder + U-Net Skip Connections.\n* **Data Pipeline:** Lazy loading from internal disk (`/tmp`) to handle large datasets without RAM explosion.\n* **Robustness:** Implements `SpatialPadd` and `DivisiblePadd` to handle variable MRI volume sizes preventing shape mismatches.\n* **Evaluation:** Clinical metrics (Dice & HD95) calculated on Whole Tumor (WT), Tumor Core (TC), and Enhancing Tumor (ET).","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# general imports\n# =============================================================================\n\nimport sys\nimport os\nimport time\nimport json\nimport tarfile\nimport subprocess\nimport warnings\nimport random\nimport shutil\nimport glob as gb\nfrom tqdm import tqdm\n\n# suppress cluttered warnings\nwarnings.filterwarnings(\"ignore\")\n\n# -----------------------------------------------------------------------------\n# mamba-ssm, monai, nibabel and einops\n# -----------------------------------------------------------------------------\nprint(\"‚öôÔ∏è Checking and Installing Dependencies...\")\nstart_install = time.time()\n\n# Helper to install if missing\ndef install_package(package_name, pip_name=None):\n    if pip_name is None: pip_name = package_name\n    try:\n        __import__(package_name)\n    except ImportError:\n        print(f\"   Installing {pip_name}...\")\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name, \"--quiet\"])\n\n# 1. Medical / 3D Imaging Libraries\ninstall_package(\"nibabel\")\ninstall_package(\"monai\")\ninstall_package(\"einops\") # needed for tensor rearranging in Mamba/Transformers\n\n# 2. Mamba-SSM & Causal Conv1d\n# We prioritize pre-built wheels to save time.\ntry:\n    import mamba_ssm\n    print(\"   ‚úÖ Mamba-SSM already installed.\")\nexcept ImportError:\n    print(\"   ‚ö†Ô∏è Mamba-SSM not found. Installing specific versions for Kaggle T4...\")\n    try:\n        # Install causal-conv1d first\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"causal-conv1d>=1.2.0\"])\n        # Install mamba-ssm\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"mamba-ssm\"])\n        print(\"   ‚úÖ Mamba-SSM installed successfully.\")\n    except Exception as e:\n        print(f\"   ‚ùå Error installing Mamba: {e}\")\n        print(\"   -> Ensure you are using GPU T4 and Internet is ON.\")\n\nprint(f\"‚úÖ Dependencies ready in {time.time() - start_install:.1f}s\")\n\n# -----------------------------------------------------------------------------\n# 1.2 other libraries\n# -----------------------------------------------------------------------------\nprint(\"üìÇ Importing Libraries...\")\n\n# > standard data science\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nfrom PIL import Image\n\n# > PyTorch & DL\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler  # mixed Precision\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n\n# > Medical / Specialized Imaging (MONAI & Nibabel)\nimport nibabel as nib\n\n# MONAI & Medical Imaging Imports\nfrom monai.utils import set_determinism\nfrom monai.losses import DiceCELoss\nfrom monai.metrics import DiceMetric, HausdorffDistanceMetric\nfrom monai.data import DataLoader, Dataset, decollate_batch\nfrom monai.transforms import (\n    Compose, LoadImaged, EnsureChannelFirstd, EnsureTyped, \n    ScaleIntensityd, RandCropByPosNegLabeld, RandFlipd, \n    RandShiftIntensityd, SpatialPadd, DivisiblePadd, AsDiscrete\n)\n\n# > Math & Tensor Manipulation\nfrom einops import rearrange, repeat\n\n# > Scikit-Learn (Metrics & Splitting)\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import accuracy_score, jaccard_score, f1_score\n\n# -----------------------------------------------------------------------------\n# 1.3 CONFIGURATION & SEEDING\n# -----------------------------------------------------------------------------\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# =============================================================================\n# ‚öôÔ∏è CONFIGURATION\n# =============================================================================\nclass Config:\n    SEED = 42\n    # Data extraction path (Using /tmp for speed and to avoid /working limits)\n    EXTRACT_PATH = \"/tmp/brats2021_data\" \n    TAR_PATH = \"/kaggle/input/brats2021-task1-converting-to-processed-dataset/BraTS2021_1000_Samples.tar\"\n    \n    # Dataset Limits (Set N_SAMPLES to None for full dataset)\n    N_SAMPLES = 1400       \n    VAL_SPLIT = 0.2\n    \n    # Training Hyperparameters\n    IMG_SIZE = (128, 128, 128)\n    BATCH_SIZE_TRAIN = 4   # Effective batch size = 8 (on 2 GPUs)\n    BATCH_SIZE_VAL = 1     # MUST be 1 to handle variable image sizes\n    NUM_WORKERS = 8        # High worker count for efficient lazy loading\n    NUM_EPOCHS = 50\n    LEARNING_RATE = 3e-4\n    MAX_RUNTIME = 11.5 * 3600 # Safety buffer for Kaggle timeout\n    \n    # Paths\n    ARTIFACTS_DIR = \"/kaggle/working/artifacts\"\n    CHECKPOINT_DIR = os.path.join(ARTIFACTS_DIR, \"checkpoints\")\n    RESULTS_DIR = os.path.join(ARTIFACTS_DIR, \"results\")\n    BEST_MODEL_DIR = os.path.join(ARTIFACTS_DIR, \"best_model\")\n\n# Create Directories\nfor d in [Config.EXTRACT_PATH, Config.CHECKPOINT_DIR, Config.RESULTS_DIR, Config.BEST_MODEL_DIR]:\n    os.makedirs(d, exist_ok=True)\n\n# Set Reproducibility\nset_determinism(seed=Config.SEED)\nprint(\"‚úÖ Configuration Complete. Artifacts will be saved to:\", Config.ARTIFACTS_DIR)\n\nprint(f\"\\nüöÄ System Ready.\")\nprint(f\"   PyTorch: {torch.__version__}\")\nprint(f\"   Device:  {DEVICE}\")\nif torch.cuda.is_available():\n    print(f\"   GPU:     {torch.cuda.get_device_name(0)}\")\n    print(f\"   Memory:  {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T13:27:33.343786Z","iopub.execute_input":"2025-12-31T13:27:33.344220Z"}},"outputs":[{"name":"stdout","text":"‚öôÔ∏è Checking and Installing Dependencies...\n   Installing monai...\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2.7/2.7 MB 28.4 MB/s eta 0:00:00\n   ‚ö†Ô∏è Mamba-SSM not found. Installing specific versions for Kaggle T4...\nCollecting causal-conv1d>=1.2.0\n  Downloading causal_conv1d-1.5.3.post1.tar.gz (24 kB)\n  Installing build dependencies: started\n  Installing build dependencies: finished with status 'done'\n  Getting requirements to build wheel: started\n  Getting requirements to build wheel: finished with status 'done'\n  Preparing metadata (pyproject.toml): started\n  Preparing metadata (pyproject.toml): finished with status 'done'\nRequirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from causal-conv1d>=1.2.0) (2.8.0+cu126)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from causal-conv1d>=1.2.0) (25.0)\nRequirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from causal-conv1d>=1.2.0) (1.13.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d>=1.2.0) (3.20.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d>=1.2.0) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d>=1.2.0) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d>=1.2.0) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d>=1.2.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d>=1.2.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d>=1.2.0) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d>=1.2.0) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d>=1.2.0) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d>=1.2.0) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d>=1.2.0) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d>=1.2.0) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d>=1.2.0) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d>=1.2.0) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d>=1.2.0) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d>=1.2.0) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d>=1.2.0) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d>=1.2.0) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d>=1.2.0) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d>=1.2.0) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d>=1.2.0) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d>=1.2.0) (3.4.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->causal-conv1d>=1.2.0) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->causal-conv1d>=1.2.0) (3.0.3)\nBuilding wheels for collected packages: causal-conv1d\n  Building wheel for causal-conv1d (pyproject.toml): started\n  Building wheel for causal-conv1d (pyproject.toml): finished with status 'done'\n  Created wheel for causal-conv1d: filename=causal_conv1d-1.5.3.post1-cp312-cp312-linux_x86_64.whl size=103623426 sha256=8bfaa81dddcbce454dfffbb8531dc8e12d3143dea5aaaf762bda04b64a2e457e\n  Stored in directory: /root/.cache/pip/wheels/50/b5/ee/ccdfcb7fa5da6970cb61695a3486c1cb4126e7050785c73ba3\nSuccessfully built causal-conv1d\nInstalling collected packages: causal-conv1d\nSuccessfully installed causal-conv1d-1.5.3.post1\nCollecting mamba-ssm\n  Downloading mamba_ssm-2.2.6.post3.tar.gz (113 kB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 113.9/113.9 kB 4.0 MB/s eta 0:00:00\n  Installing build dependencies: started\n  Installing build dependencies: finished with status 'done'\n  Getting requirements to build wheel: started\n  Getting requirements to build wheel: finished with status 'done'\n  Preparing metadata (pyproject.toml): started\n  Preparing metadata (pyproject.toml): finished with status 'done'\nRequirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from mamba-ssm) (2.8.0+cu126)\nRequirement already satisfied: triton in /usr/local/lib/python3.12/dist-packages (from mamba-ssm) (3.4.0)\nRequirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from mamba-ssm) (1.13.0)\nRequirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from mamba-ssm) (0.8.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from mamba-ssm) (4.57.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from mamba-ssm) (25.0)\nRequirement already satisfied: setuptools>=61.0.0 in /usr/local/lib/python3.12/dist-packages (from mamba-ssm) (75.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (3.20.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (4.15.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (1.11.1.6)\nRequirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm) (2.0.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm) (2.32.5)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm) (0.6.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm) (4.67.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers->mamba-ssm) (1.2.1rc0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->mamba-ssm) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->mamba-ssm) (3.0.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->mamba-ssm) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->mamba-ssm) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->mamba-ssm) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->mamba-ssm) (2025.11.12)\nBuilding wheels for collected packages: mamba-ssm\n  Building wheel for mamba-ssm (pyproject.toml): started\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## DataSet - BraTS (Brain Tumor Segmentation)","metadata":{}},{"cell_type":"markdown","source":"**Why this Dataset?**\n\nWe utilize the **BraTS Challenge Dataset**, the global benchmark for medical 3D segmentation. This dataset is uniquely suited for **BratMamba** because:\n\n1. **3D Volumetric Data**: Unlike 2D datasets, BraTS provides full 3D MRI volumes ($240 \\times 240 \\times 155$). This justifies the use of Mamba-SSM, which excels at modeling the extremely long sequences created by flattening 3D volumes ($N = H \\times W \\times D$), a task where Transformers typically run out of memory.\n2. **Multi-Modal Complexity**: Each patient has 4 modalities (T1, T1c, T2, FLAIR). Our \"Dual CNN Stem\" is designed specifically to fuse these heterogeneous signals locally before global processing.\n3. **Class Imbalance**: The tumor sub-regions (Necrosis, Edema, Enhancing) vary wildly in size, requiring robust loss functions (Dice/Focal) rather than simple accuracy.\n  \n\n*Segmentation Classes & Labels*\n\nWe follow the standard BraTS protocol:\n* Label 0: Background\n* Label 1 (NCR): Necrotic Tumor Core (Hypointense on T1-Gd)\n* Label 2 (ED): Peritumoral Edema (Hyperintense on FLAIR)\n* Label 4 (ET): Enhancing Tumor (Hyperintense on T1-Gd)\n\n\n*Dataset Paper Citation*:\n\n[1]U. Baid et al., ‚ÄúThe RSNA-ASNR-MICCAI BraTS 2021 Benchmark on Brain Tumor Segmentation and Radiogenomic Classification,‚Äù arXiv:2107.02314 [cs], Sep. 2021, Available: https://arxiv.org/abs/2107.02314\n\n*Evaluation Metrics*\n\nTo ensure fair comparison with SOTA, we track:\n1. **Dice Similarity Coefficient (DSC)**: Measures overlap accuracy.\n2. **Hausdorff Distance (HD95)**: Measures the worst-case boundary error (critical for surgical planning).","metadata":{}},{"cell_type":"markdown","source":"## Importing Dataset & Preparation for training\n\n\nWe already process the BraTS 2021 Task01 dataset on `brats2021-task1-converting-to-processed-dataset` notebook and imported all the processed data to this notebook to save disk space.","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# üì¶ DATA UNPACKING & DISCOVERY\n# =============================================================================\ndef unpack_dataset(tar_path, extract_path, limit=None):\n    \"\"\"Extracts dataset from TAR to fast local storage.\"\"\"\n    print(f\"üì¶ Source: {tar_path}\")\n    \n    # Check existing files to avoid re-extracting\n    existing = gb.glob(os.path.join(extract_path, \"**\", \"*_x.npy\"), recursive=True)\n    if limit and len(existing) >= limit:\n        print(f\"‚úÖ Data already unpacked ({len(existing)} samples). Skipping...\")\n        return\n\n    print(\"‚è≥ Unpacking... (This utilizes internal disk IO)\")\n    with tarfile.open(tar_path, \"r\") as tar:\n        members = tar.getmembers()\n        count = 0\n        for member in tqdm(members, desc=\"Extracting\"):\n            if limit and count >= limit: break\n            tar.extract(member, path=extract_path)\n            count += 1\n    print(f\"‚úÖ Extracted {count} files.\")\n\ndef get_file_lists(data_dir):\n    \"\"\"Robustly pairs Input volumes (*_x.npy) with Label volumes (*_y.npy).\"\"\"\n    print(f\"üîç Scanning: {data_dir}\")\n    input_files = sorted(gb.glob(os.path.join(data_dir, \"**\", \"*_x.npy\"), recursive=True))\n    label_files = sorted(gb.glob(os.path.join(data_dir, \"**\", \"*_y.npy\"), recursive=True))\n    \n    if not input_files:\n        raise ValueError(\"‚ùå No .npy files found! Check dataset path.\")\n        \n    data_dicts = [{\"image\": i, \"label\": l} for i, l in zip(input_files, label_files)]\n    return data_dicts[:Config.N_SAMPLES]\n\n# Execute Unpack\nunpack_dataset(Config.TAR_PATH, Config.EXTRACT_PATH, limit=Config.N_SAMPLES)\nall_files = get_file_lists(Config.EXTRACT_PATH)\n\n# Split Data\nval_count = int(len(all_files) * Config.VAL_SPLIT)\nval_count = max(1, val_count) # Ensure at least 1 val sample\ntrain_files, val_files = all_files[val_count:], all_files[:val_count]\n\nprint(f\"üìä Dataset Split: Train={len(train_files)} | Val={len(val_files)}\")\n\n# =============================================================================\n# üîÑ TRANSFORMS & LOADERS\n# =============================================================================\n# Training: Aggressive Augmentation + Fixed Cropping\ntrain_transforms = Compose([\n    LoadImaged(keys=[\"image\", \"label\"]),\n    EnsureChannelFirstd(keys=[\"image\"], channel_dim=0),\n    EnsureTyped(keys=[\"image\", \"label\"]),\n    ScaleIntensityd(keys=[\"image\"]),\n    # Safety: Pad small images to avoid crash during crop\n    SpatialPadd(keys=[\"image\", \"label\"], spatial_size=Config.IMG_SIZE, method='symmetric'),\n    RandCropByPosNegLabeld(\n        keys=[\"image\", \"label\"], label_key=\"label\",\n        spatial_size=Config.IMG_SIZE, pos=1, neg=1, num_samples=1,\n        image_key=\"image\", image_threshold=0,\n    ),\n    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n    RandShiftIntensityd(keys=[\"image\"], offsets=0.1, prob=0.5),\n])\n\n# Validation: Full Volume (No Crop) + Divisible Padding\nval_transforms = Compose([\n    LoadImaged(keys=[\"image\", \"label\"]),\n    EnsureChannelFirstd(keys=[\"image\"], channel_dim=0),\n    EnsureTyped(keys=[\"image\", \"label\"]),\n    ScaleIntensityd(keys=[\"image\"]),\n    # Safety 1: Pad to minimum size\n    SpatialPadd(keys=[\"image\", \"label\"], spatial_size=Config.IMG_SIZE, method='symmetric'),\n    # Safety 2: Pad to be divisible by 16 (Required for U-Net/Mamba downsampling)\n    DivisiblePadd(keys=[\"image\", \"label\"], k=16)\n])\n\nprint(\"‚è≥ Initializing Loaders (Lazy Loading)...\")\ntrain_ds = Dataset(data=train_files, transform=train_transforms)\nval_ds = Dataset(data=val_files, transform=val_transforms)\n\ntrain_loader = DataLoader(\n    train_ds, batch_size=Config.BATCH_SIZE_TRAIN, shuffle=True, \n    num_workers=Config.NUM_WORKERS, pin_memory=True\n)\n# Note: Batch Size 1 is critical for Val to handle variable volume sizes\nval_loader = DataLoader(\n    val_ds, batch_size=Config.BATCH_SIZE_VAL, shuffle=False, \n    num_workers=Config.NUM_WORKERS, pin_memory=True\n)\nprint(\"‚úÖ Loaders Ready.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualization","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# üëÅÔ∏è DATA SANITY CHECK\n# =============================================================================\ndef visualize_batch(loader, save_path=None):\n    print(\"‚è≥ Fetching batch for visualization...\")\n    try:\n        batch = next(iter(loader))\n    except Exception as e:\n        print(f\"‚ùå Error fetching batch: {e}\"); return\n\n    images, masks = batch[\"image\"], batch[\"label\"]\n    print(f\"   Batch Shape: {images.shape}\")\n    \n    # Search for a sample with tumor content\n    sample_idx = 0\n    for i in range(len(images)):\n        if masks[i].sum() > 0:\n            sample_idx = i; break\n    \n    img_t = images[sample_idx]\n    msk_t = masks[sample_idx]\n    if msk_t.ndim == 4: msk_t = msk_t[0] # Handle channel dim\n    \n    # Find Axial Slice with Max Tumor Area\n    tumor_counts = torch.sum(msk_t > 0, dim=(1, 2))\n    slice_idx = torch.argmax(tumor_counts).item()\n    if tumor_counts.max() == 0: slice_idx = img_t.shape[1] // 2\n    \n    print(f\"‚úÖ Visualizing Sample {sample_idx}, Slice {slice_idx}\")\n\n    # Plot\n    slice_img = img_t[:, slice_idx, :, :].cpu().numpy()\n    slice_msk = msk_t[slice_idx, :, :].cpu().numpy()\n    \n    fig, ax = plt.subplots(1, 5, figsize=(20, 5))\n    modes = [\"T1\", \"T1ce\", \"T2\", \"FLAIR\", \"Mask\"]\n    for i in range(4):\n        ax[i].imshow(slice_img[i], cmap=\"gray\")\n        ax[i].set_title(modes[i]); ax[i].axis(\"off\")\n    \n    ax[4].imshow(slice_msk, cmap=\"jet\", vmin=0, vmax=3)\n    ax[4].set_title(\"Ground Truth\"); ax[4].axis(\"off\")\n    \n    if save_path: plt.savefig(save_path)\n    plt.show()\n\nvisualize_batch(train_loader, save_path=os.path.join(Config.ARTIFACTS_DIR, \"sanity_check.png\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Architecture: BratMamba","metadata":{}},{"cell_type":"markdown","source":"                           ~Fusing Local Texture with Global Context~","metadata":{}},{"cell_type":"markdown","source":"Our architecture, **BratMamba**, addresses the limitations of purely Convolutional networks (like nnU-Net) and purely Transformer-based networks (like Swin UNETR) by leveraging the **Linear Complexity ($O(N)$)** of State Space Models (Mamba).\n\n*4.1 Key Design Decisions & Citations*\n\n1. **The Dual-Stage CNN Stem (Our Innovation)**\n      * **The Component**: Instead of a single $7 \\times 7$ patch embedding (like in Swin UNETR), we split the input into two parallel paths: one with a small kernel ($3 \\times 3$) and one with a large kernel ($7 \\times 7$).\n          * **Stream A ($3 \\times 3$ Kernel)**: Captures high-frequency details (sharp edges of the Necrotic Core).\n          * **Stream B ($7 \\times 7$ Kernel)**: Captures low-frequency context (large Edema regions).\n          * **Fusion**: These are concatenated to give the Mamba blocks a \"rich\" feature set that contains both texture and context. \n      * **The Reason**:\n          * **Local Texture**: MRI Brain tumor boundaries are defined by subtle texture changes (Necrosis vs. Edema). Small kernels capture these high-frequency edges.\n          * **Receptive Field**: Large kernels capture the \"neighborhood\" context immediately.\n          * **Paper Reference**: *Swin UNETR* (Hatamizadeh et al., 2022) highlights the importance of patch merging, but notes that Transformers often lose local spatial details early on. Our Dual-Stem preserves this before the Mamba layers take over.\n          \n2. **The Mamba Encoder (The Engine)**\n      * **The Component**: Stacked Mamba Blocks that flatten the 3D volume into a 1D sequence.\n      * **The Reason**:\n          * **The Problem**: Standard Self-Attention (Transformers) scales quadratically ($N^2$). For a 3D volume of $128^3$, the sequence length is ~2 million. A Transformer would run out of memory immediately.\n          * **The Solution**: Mamba scales linearly ($N$). It allows us to scan the entire 3D brain volume as a single sequence, understanding that \"a pixel at the top left\" is related to \"a pixel at the bottom right\" (Global Context).\n            * **Global Scanning**: The Mamba block scans the image left-to-right, right-to-left, and top-to-bottom, effectively \"seeing\" the whole brain at once to decide if a pixel is tumor or noise. \n          * **Paper Reference**: SegMamba (Xing et al., 2024) demonstrates that Mamba outperforms Transformers on 3D medical data by reducing memory usage by 60% while improving Dice scores on the BraTS dataset.\n\n3. **Deep Supervision & Skip Connections**\n    * **The Component**: Direct connections between the Encoder and Decoder at matching resolutions.\n    * **The Reason**: As the network goes deeper to understand \"shapes\" (Tumor vs Brain), it loses spatial resolution. Skip connections inject the high-resolution texture details from the Stem directly into the Decoder, ensuring the final mask has sharp edges.\n    * **Paper Reference**: nnU-Net (Isensee et al., 2020) proves that robust encoder-decoder connections are often more important than the choice of optimizer or activation function.","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# üß† MODEL ARCHITECTURE: BRATMAMBA\n# =============================================================================\ntry:\n    from mamba_ssm import Mamba\nexcept ImportError:\n    print(\"‚ö†Ô∏è WARNING: mamba_ssm not found. Using Mock layer (Install for real training!)\")\n    class Mamba(nn.Module):\n        def __init__(self, d_model, d_state, d_conv, expand): super().__init__()\n        def forward(self, x): return x\n\nclass DualConvStem(nn.Module):\n    \"\"\"Hybrid Stem: Captures fine details (3x3) and coarse context (7x7).\"\"\"\n    def __init__(self, in_chans, out_chans):\n        super().__init__()\n        self.branch1 = nn.Sequential(\n            nn.Conv3d(in_chans, out_chans // 2, kernel_size=3, padding=1, stride=2),\n            nn.InstanceNorm3d(out_chans // 2), nn.GELU()\n        )\n        self.branch2 = nn.Sequential(\n            nn.Conv3d(in_chans, out_chans // 2, kernel_size=7, padding=3, stride=2),\n            nn.InstanceNorm3d(out_chans // 2), nn.GELU()\n        )\n        self.fusion = nn.Conv3d(out_chans, out_chans, kernel_size=1)\n\n    def forward(self, x):\n        return self.fusion(torch.cat([self.branch1(x), self.branch2(x)], dim=1))\n\nclass MambaLayer(nn.Module):\n    \"\"\"Volumetric Mamba Block: Flattens 3D -> Sequence -> Mamba -> 3D.\"\"\"\n    def __init__(self, dim, d_state=16, d_conv=4, expand=2):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.mamba = Mamba(d_model=dim, d_state=d_state, d_conv=d_conv, expand=expand)\n    \n    def forward(self, x):\n        B, C, D, H, W = x.shape\n        x_flat = x.flatten(2).transpose(1, 2)\n        x_mamba = self.mamba(self.norm(x_flat))\n        out = x_flat + x_mamba\n        return out.transpose(1, 2).view(B, C, D, H, W)\n\nclass UpBlock(nn.Module):\n    \"\"\"Standard U-Net Decoder Block.\"\"\"\n    def __init__(self, in_chans, out_chans):\n        super().__init__()\n        self.up = nn.ConvTranspose3d(in_chans, out_chans, kernel_size=2, stride=2)\n        self.conv = nn.Sequential(\n            nn.Conv3d(out_chans * 2, out_chans, kernel_size=3, padding=1),\n            nn.InstanceNorm3d(out_chans), nn.GELU()\n        )\n\n    def forward(self, x, skip):\n        x = self.up(x)\n        if x.shape != skip.shape: # Handle padding mismatches\n            x = nn.functional.interpolate(x, size=skip.shape[2:], mode='trilinear')\n        return self.conv(torch.cat([x, skip], dim=1))\n\nclass BratMamba(nn.Module):\n    def __init__(self, in_chans=4, num_classes=4, embed_dim=48):\n        super().__init__()\n        self.stem = DualConvStem(in_chans, embed_dim)\n        self.layer1 = MambaLayer(embed_dim)\n        self.down1 = nn.Conv3d(embed_dim, embed_dim*2, kernel_size=3, stride=2, padding=1)\n        self.layer2 = MambaLayer(embed_dim*2)\n        self.down2 = nn.Conv3d(embed_dim*2, embed_dim*4, kernel_size=3, stride=2, padding=1)\n        self.bottleneck = MambaLayer(embed_dim*4)\n        \n        self.up1 = UpBlock(embed_dim*4, embed_dim*2)\n        self.up2 = UpBlock(embed_dim*2, embed_dim)\n        self.final_up = nn.ConvTranspose3d(embed_dim, embed_dim, kernel_size=2, stride=2)\n        self.out_head = nn.Conv3d(embed_dim, num_classes, kernel_size=1)\n\n    def forward(self, x):\n        x1 = self.layer1(self.stem(x))\n        x2 = self.layer2(self.down1(x1))\n        x3 = self.bottleneck(self.down2(x2))\n        d1 = self.up1(x3, x2)\n        d2 = self.up2(d1, x1)\n        return self.out_head(self.final_up(d2))\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = BratMamba().to(DEVICE)\nprint(f\"‚úÖ Model Initialized on {DEVICE}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training Loop","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# üöÄ TRAINING LOOP\n# =============================================================================\nGPU_COUNT = torch.cuda.device_count()\nif GPU_COUNT > 1:\n    print(f\"‚ö° Using {GPU_COUNT} GPUs (DataParallel)\")\n    model = nn.DataParallel(model)\n\ncriterion = DiceCELoss(to_onehot_y=True, softmax=True, include_background=False)\noptimizer = optim.AdamW(model.parameters(), lr=Config.LEARNING_RATE, weight_decay=1e-5)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=Config.NUM_EPOCHS)\nscaler = GradScaler('cuda') \ndice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n\n# Resume Logic\nhistory = {'train_loss': [], 'val_loss': [], 'val_dice': [], 'epochs': []}\nstart_epoch, best_dice = 0, 0.0\nlast_ckpt = os.path.join(Config.CHECKPOINT_DIR, \"last.pth\")\nhistory_path = os.path.join(Config.RESULTS_DIR, \"log.json\")\n\nif os.path.exists(last_ckpt):\n    print(\"üîÑ Resuming from checkpoint...\")\n    ckpt = torch.load(last_ckpt)\n    \n    # Load State Dict handling DataParallel wrapper\n    sd = ckpt['model_state_dict']\n    if isinstance(model, nn.DataParallel): model.module.load_state_dict(sd)\n    else: \n        # Fix if resuming non-parallel on parallel or vice-versa\n        new_sd = {k.replace('module.', ''): v for k, v in sd.items()}\n        model.load_state_dict(new_sd, strict=False)\n        \n    optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n    scaler.load_state_dict(ckpt['scaler_state_dict'])\n    start_epoch = ckpt['epoch'] + 1\n    best_dice = ckpt['best_dice']\n    if os.path.exists(history_path):\n        with open(history_path, 'r') as f: history = json.load(f)\n\nprint(f\"üöÄ Training starting at Epoch {start_epoch+1}...\")\nSTART_TIME = time.time()\n\nfor epoch in range(start_epoch, Config.NUM_EPOCHS):\n    if time.time() - START_TIME > Config.MAX_RUNTIME:\n        print(\"üõë Time limit reached.\"); break\n        \n    model.train()\n    ep_loss = 0\n    \n    # Train Step\n    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Ep {epoch+1}\")\n    for i, batch in pbar:\n        img, lbl = batch[\"image\"].to(DEVICE), batch[\"label\"].to(DEVICE)\n        \n        optimizer.zero_grad()\n        with autocast('cuda'):\n            pred = model(img)\n            loss = criterion(pred, lbl)\n        \n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        ep_loss += loss.item()\n        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n\n    # Val Step\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Val\"):\n            img, lbl = batch[\"image\"].to(DEVICE), batch[\"label\"].to(DEVICE)\n            with autocast('cuda'):\n                pred = model(img)\n                val_loss += criterion(pred, lbl).item()\n            \n            p = [AsDiscrete(argmax=True, to_onehot=4)(i) for i in pred]\n            t = [AsDiscrete(to_onehot=4)(i) for i in lbl]\n            dice_metric(y_pred=p, y=t)\n            \n    # Stats\n    stats = {\n        'train_loss': ep_loss / len(train_loader),\n        'val_loss': val_loss / len(val_loader),\n        'val_dice': dice_metric.aggregate().item()\n    }\n    dice_metric.reset()\n    \n    # Update History & Log\n    history['epochs'].append(epoch+1)\n    for k, v in stats.items(): history[k].append(v)\n    with open(history_path, 'w') as f: json.dump(history, f)\n    \n    print(f\"   Stats: Train={stats['train_loss']:.4f} | Val={stats['val_loss']:.4f} | Dice={stats['val_dice']:.4f}\")\n    \n    # Save State\n    state = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n    ckpt = {\n        'epoch': epoch, 'model_state_dict': state,\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scaler_state_dict': scaler.state_dict(), 'best_dice': best_dice\n    }\n    torch.save(ckpt, last_ckpt)\n    \n    if stats['val_dice'] > best_dice:\n        print(f\"   ‚≠ê New Best! {best_dice:.4f} -> {stats['val_dice']:.4f}\")\n        best_dice = stats['val_dice']\n        torch.save(state, os.path.join(Config.BEST_MODEL_DIR, \"best_model.pth\"))\n        \n    scheduler.step()\n\n# Plot\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1); plt.plot(history['train_loss'], label='Train'); plt.plot(history['val_loss'], label='Val'); plt.legend()\nplt.subplot(1, 2, 2); plt.plot(history['val_dice'], label='Dice', color='green'); plt.legend()\nplt.savefig(os.path.join(Config.RESULTS_DIR, \"training_curves.png\"))\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation ","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# üèÜ EVALUATION & METRICS\n# =============================================================================\ndef get_brats_regions(tensor_oh):\n    \"\"\"Converts One-Hot (BG, NCR, ED, ET) -> (WT, TC, ET)\"\"\"\n    wt = torch.sum(tensor_oh[:, 1:4, ...], dim=1, keepdim=True) > 0\n    tc = (tensor_oh[:, 1:2, ...] + tensor_oh[:, 3:4, ...]) > 0\n    et = tensor_oh[:, 3:4, ...] > 0\n    return torch.cat([wt, tc, et], dim=1).float()\n\ndef evaluate_model(model, loader):\n    model.eval()\n    dice_metric = DiceMetric(include_background=True, reduction=\"mean_batch\")\n    hd95_metric = HausdorffDistanceMetric(include_background=True, percentile=95, reduction=\"mean_batch\")\n    post_pred = AsDiscrete(argmax=True, to_onehot=4)\n    post_label = AsDiscrete(to_onehot=4)\n    \n    print(\"üîç Starting Final Evaluation...\")\n    with torch.no_grad():\n        for i, batch in tqdm(enumerate(loader), total=len(loader)):\n            img, lbl = batch[\"image\"].to(DEVICE), batch[\"label\"].to(DEVICE)\n            pred = model(img)\n            \n            # Post-process\n            pred_oh = torch.stack([post_pred(x) for x in pred])\n            lbl_oh = torch.stack([post_label(x) for x in lbl])\n            \n            # Convert to BraTS Regions\n            pred_reg = get_brats_regions(pred_oh)\n            lbl_reg = get_brats_regions(lbl_oh)\n            \n            dice_metric(y_pred=pred_reg, y=lbl_reg)\n            hd95_metric(y_pred=pred_reg, y=lbl_reg)\n            \n            # Visualize First Batch\n            if i == 0:\n                visualize_prediction(img[0], lbl[0], pred[0], \n                                     os.path.join(Config.RESULTS_DIR, \"best_pred.png\"))\n\n    # Aggregate\n    dice = dice_metric.aggregate().cpu().numpy()\n    hd95 = hd95_metric.aggregate().cpu().numpy()\n    \n    df = pd.DataFrame({\n        \"Region\": [\"Whole Tumor (WT)\", \"Tumor Core (TC)\", \"Enhancing Tumor (ET)\"],\n        \"Dice (DSC) ‚Üë\": dice,\n        \"HD95 (mm) ‚Üì\": hd95,\n        \"Mean Dice\": [dice.mean()] * 3\n    })\n    \n    csv_path = os.path.join(Config.RESULTS_DIR, \"final_metrics.csv\")\n    df.to_csv(csv_path, index=False)\n    print(\"\\nüèÜ FINAL SCORES:\"); print(df.to_string(index=False, float_format=\"%.4f\"))\n\ndef visualize_prediction(img, lbl, pred, save_path):\n    \"\"\"Helper to visualize input vs prediction.\"\"\"\n    vol = lbl.sum(dim=(0,1,2)); idx = torch.argmax(vol).item()\n    if vol.max() == 0: idx = img.shape[2] // 2\n    \n    im = img[2, :, :, idx].cpu().numpy()\n    gt = torch.argmax(lbl, dim=0)[:, :, idx].cpu().numpy()\n    pr = torch.argmax(pred, dim=0)[:, :, idx].cpu().numpy()\n    \n    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n    ax[0].imshow(im, cmap='gray'); ax[0].set_title(\"Input (T2)\")\n    ax[1].imshow(gt, cmap='jet', vmin=0, vmax=3); ax[1].set_title(\"GT\")\n    ax[2].imshow(pr, cmap='jet', vmin=0, vmax=3); ax[2].set_title(\"Pred\")\n    plt.savefig(save_path); plt.close()\n\n# Load Best & Run\nbest_path = os.path.join(Config.BEST_MODEL_DIR, \"best_model.pth\")\nif os.path.exists(best_path):\n    print(f\"üìÇ Loading Best Model: {best_path}\")\n    sd = torch.load(best_path)\n    if isinstance(model, nn.DataParallel): model.module.load_state_dict(sd)\n    else: model.load_state_dict(sd)\n    evaluate_model(model, val_loader)\nelse:\n    print(\"‚ö†Ô∏è No best model found.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}